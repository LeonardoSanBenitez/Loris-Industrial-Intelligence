{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "basic multi-step lstm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngduuYZW8ogs",
        "colab_type": "text"
      },
      "source": [
        "Adopted from https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjQNvygZ8ogx",
        "colab_type": "code",
        "outputId": "cc4d48cd-9925-49dd-aa0b-f98f04f14c1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# load and clean-up data\n",
        "from numpy import nan\n",
        "from numpy import isnan\n",
        "from pandas import read_csv\n",
        "from pandas import to_numeric\n",
        "from numpy import split\n",
        "from numpy import array\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import ConvLSTM2D"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmjtJTpP9GYX",
        "colab_type": "text"
      },
      "source": [
        "# Pre processing\n",
        "* Fill missing values\n",
        "* Downsample to days\n",
        "* Overlaping Windows of weekly groups\n",
        "* Batch by week"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnY77Gda8yEj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "8637aced-2a9d-4b5e-84db-798cb131fb07"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip\n",
        "!unzip household_power_consumption.zip\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-29 01:34:40--  https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20640916 (20M) [application/x-httpd-php]\n",
            "Saving to: ‘household_power_consumption.zip.2’\n",
            "\n",
            "\r          household   0%[                    ]       0  --.-KB/s               \r         household_   7%[>                   ]   1.42M  7.03MB/s               \r        household_p  98%[==================> ]  19.38M  48.1MB/s               \rhousehold_power_con 100%[===================>]  19.68M  48.4MB/s    in 0.4s    \n",
            "\n",
            "2019-08-29 01:34:40 (48.4 MB/s) - ‘household_power_consumption.zip.2’ saved [20640916/20640916]\n",
            "\n",
            "Archive:  household_power_consumption.zip\n",
            "replace household_power_consumption.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: household_power_consumption.txt  \n",
            "household_power_consumption.csv       household_power_consumption.zip.1\n",
            "household_power_consumption_days.csv  household_power_consumption.zip.2\n",
            "household_power_consumption.txt       sample_data\n",
            "household_power_consumption.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "zAT802UM8ohA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fill missing values with a value at the same time one day ago\n",
        "def fill_missing(values):\n",
        "    one_day = 60 * 24\n",
        "    for row in range(values.shape[0]):\n",
        "        for col in range(values.shape[1]):\n",
        "            if isnan(values[row, col]):\n",
        "                values[row, col] = values[row - one_day, col]\n",
        "\n",
        "# load all data\n",
        "dataset = read_csv('household_power_consumption.txt', sep=';', header=0, low_memory=False, infer_datetime_format=True, parse_dates={'datetime':[0,1]}, index_col=['datetime'])\n",
        "# mark all missing values\n",
        "dataset.replace('?', nan, inplace=True)\n",
        "# make dataset numeric\n",
        "dataset = dataset.astype('float32')\n",
        "# fill missing\n",
        "fill_missing(dataset.values)\n",
        "# add a column for for the remainder of sub metering\n",
        "values = dataset.values\n",
        "dataset['sub_metering_4'] = (values[:,0] * 1000 / 60) - (values[:,4] + values[:,5] + values[:,6])\n",
        "# save updated dataset\n",
        "dataset.to_csv('household_power_consumption.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "HkkYNrTU8ohJ",
        "colab_type": "code",
        "outputId": "0abe1676-3f81-456b-e731-ce9e806eac7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "# resample minute data to total for each day\n",
        "# load the new file\n",
        "dataset = read_csv('household_power_consumption.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n",
        "# resample data to daily\n",
        "daily_groups = dataset.resample('D')\n",
        "daily_data = daily_groups.sum()\n",
        "# summarize\n",
        "print(daily_data.shape)\n",
        "print(daily_data.head())\n",
        "# save\n",
        "daily_data.to_csv('household_power_consumption_days.csv')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1442, 8)\n",
            "            Global_active_power  ...  sub_metering_4\n",
            "datetime                         ...                \n",
            "2006-12-16             1209.176  ...    14680.933319\n",
            "2006-12-17             3390.460  ...    36946.666732\n",
            "2006-12-18             2203.826  ...    19028.433281\n",
            "2006-12-19             1666.194  ...    13131.900043\n",
            "2006-12-20             2225.748  ...    20384.800011\n",
            "\n",
            "[5 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCjYSQqq8ohS",
        "colab_type": "code",
        "outputId": "71fa623d-42c5-42c3-8e92-d3efdd1e0cad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# split into standard weeks\n",
        "\n",
        "# split a univariate dataset into train/test sets\n",
        "def split_dataset(data):\n",
        "    # split into standard weeks\n",
        "    train, test = data[1:-328], data[-328:-6]\n",
        "    # restructure into windows of weekly data\n",
        "    train = array(split(train, len(train)/7))\n",
        "    test = array(split(test, len(test)/7))\n",
        "    return train, test\n",
        "\n",
        "# load the new file\n",
        "dataset = read_csv('household_power_consumption_days.csv', header=0, infer_datetime_format=True, parse_dates=['datetime'], index_col=['datetime'])\n",
        "train, test = split_dataset(dataset.values)\n",
        "# validate train data\n",
        "print(train.shape)\n",
        "print(train[0, 0, 0], train[-1, -1, 0])\n",
        "# validate test\n",
        "print(test.shape)\n",
        "print(test[0, 0, 0], test[-1, -1, 0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(159, 7, 8)\n",
            "3390.46 1309.2679999999998\n",
            "(46, 7, 8)\n",
            "2083.4539999999984 2197.006000000004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXETKN4W8ohZ",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Model With Univariate Input and Vector Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIvaVGpj8ohc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# evaluate one or more weekly forecasts against expected values\n",
        "def evaluate_forecasts(actual, predicted):\n",
        "    scores = list()\n",
        "    # calculate an RMSE score for each day\n",
        "    for i in range(actual.shape[1]):\n",
        "        # calculate mse\n",
        "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
        "        # calculate rmse\n",
        "        rmse = sqrt(mse)\n",
        "        # store\n",
        "        scores.append(rmse)\n",
        "    # calculate overall RMSE\n",
        "    s = 0\n",
        "    for row in range(actual.shape[0]):\n",
        "        for col in range(actual.shape[1]):\n",
        "            s += (actual[row, col] - predicted[row, col])**2\n",
        "    score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
        "    return score, scores\n",
        "\n",
        "# summarize scores\n",
        "def summarize_scores(name, score, scores):\n",
        "    s_scores = ', '.join(['%.1f' % s for s in scores])\n",
        "    print('%s: [%.3f] %s' % (name, score, s_scores))\n",
        "\n",
        "\n",
        "# evaluate a single model\n",
        "def evaluate_model(train, test, n_input):\n",
        "    # fit model\n",
        "    model = build_model(train, n_input)\n",
        "    # history is a list of weekly data\n",
        "    history = [x for x in train]\n",
        "    # walk-forward validation over each week\n",
        "    predictions = list()\n",
        "    for i in range(len(test)):\n",
        "        # predict the week\n",
        "        yhat_sequence = forecast(model, history, n_input)\n",
        "        # store the predictions\n",
        "        predictions.append(yhat_sequence)\n",
        "        # get real observation and add to history for predicting the next week\n",
        "        history.append(test[i, :])\n",
        "    # evaluate predictions days for each week\n",
        "    predictions = array(predictions)\n",
        "    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
        "    return score, scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YosIGEiQ8oic",
        "colab_type": "text"
      },
      "source": [
        "## Encoder-Decoder LSTM Model With Multivariate Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpx-uipd8oid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert history into inputs and outputs\n",
        "def to_supervised(train, n_input, n_out=7):\n",
        "    # flatten data\n",
        "    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
        "    X, y = list(), list()\n",
        "    in_start = 0\n",
        "    # step over the entire history one time step at a time\n",
        "    for _ in range(len(data)):\n",
        "        # define the end of the input sequence\n",
        "        in_end = in_start + n_input\n",
        "        out_end = in_end + n_out\n",
        "        # ensure we have enough data for this instance\n",
        "        if out_end < len(data):\n",
        "            X.append(data[in_start:in_end, :])\n",
        "            y.append(data[in_end:out_end, 0])\n",
        "        # move along one time step\n",
        "        in_start += 1\n",
        "    return array(X), array(y)\n",
        " \n",
        "# train the model\n",
        "def build_model(train, n_input):\n",
        "    # prepare data\n",
        "    train_x, train_y = to_supervised(train, n_input)\n",
        "    # define parameters\n",
        "    verbose, epochs, batch_size = 1, 50, 16\n",
        "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
        "    # reshape output intodel = Sequential()\n",
        "    model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
        "    model.add(RepeatVector(n_outputs))\n",
        "    model.add(LSTM(200, activation='relu', return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(100, activation='relu')))\n",
        "    model.add(TimeDistributed(Dense(1)))\n",
        "    model.compile(loss='mse', optimizer='adam')o [samples, timesteps, features]\n",
        "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
        "    # define model\n",
        "    m\n",
        "    # fit network\n",
        "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "    return model\n",
        " \n",
        "# make a forecast\n",
        "def forecast(model, history, n_input):\n",
        "    # flatten data\n",
        "    data = array(history)\n",
        "    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
        "    # retrieve last observations for input data\n",
        "    input_x = data[-n_input:, :]\n",
        "    # reshape into [1, n_input, n]\n",
        "    input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
        "    # forecast the next week\n",
        "    yhat = model.predict(input_x, verbose=0)\n",
        "    # we only want the vector forecast\n",
        "    yhat = yhat[0]\n",
        "    return yhat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnTLnLtr8oii",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3001596-7ae2-4e26-d792-61b8e6b307a7"
      },
      "source": [
        "# evaluate model and get scores\n",
        "n_input = 14\n",
        "score, scores = evaluate_model(train, test, n_input)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0829 01:35:43.681224 140308922886016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0829 01:35:43.695527 140308922886016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0829 01:35:43.700978 140308922886016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0829 01:35:44.243808 140308922886016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0829 01:35:44.491538 140308922886016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0829 01:35:45.629821 140308922886016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "W0829 01:35:45.764440 140308922886016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1092/1092 [==============================] - 5s 5ms/step - loss: 70198929.2747\n",
            "Epoch 2/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 17801250.2692\n",
            "Epoch 3/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 30534912.9886\n",
            "Epoch 4/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 9557669.4826\n",
            "Epoch 5/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 9601723.3755\n",
            "Epoch 6/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 494840.9578\n",
            "Epoch 7/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 968144.1209\n",
            "Epoch 8/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 411801.9886\n",
            "Epoch 9/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 371059.1783\n",
            "Epoch 10/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 323876.6389\n",
            "Epoch 11/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 324740.2802\n",
            "Epoch 12/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 471349.9725\n",
            "Epoch 13/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 594440.9464\n",
            "Epoch 14/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 395849.8590\n",
            "Epoch 15/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 469844.3338\n",
            "Epoch 16/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 4492457.0224\n",
            "Epoch 17/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 17309224.9148\n",
            "Epoch 18/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 688706.1474\n",
            "Epoch 19/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 727177.8278\n",
            "Epoch 20/50\n",
            "1092/1092 [==============================] - 3s 3ms/step - loss: 349985.7214\n",
            "Epoch 21/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 370190.0187\n",
            "Epoch 22/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 352965.5314\n",
            "Epoch 23/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 345312.5358\n",
            "Epoch 24/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 343383.3500\n",
            "Epoch 25/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 357623.5419\n",
            "Epoch 26/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 342604.9385\n",
            "Epoch 27/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 307467.1440\n",
            "Epoch 28/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 305867.9951\n",
            "Epoch 29/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 304842.9275\n",
            "Epoch 30/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 292484.5682\n",
            "Epoch 31/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 308989.1382\n",
            "Epoch 32/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 308556.2448\n",
            "Epoch 33/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 282958.7842\n",
            "Epoch 34/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 283881.0957\n",
            "Epoch 35/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 319784.9174\n",
            "Epoch 36/50\n",
            "1092/1092 [==============================] - 3s 3ms/step - loss: 344363.1708\n",
            "Epoch 37/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 271857.0628\n",
            "Epoch 38/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 271367.1924\n",
            "Epoch 39/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 308692.4464\n",
            "Epoch 40/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 283481.0134\n",
            "Epoch 41/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 268384.6884\n",
            "Epoch 42/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 278455.2395\n",
            "Epoch 43/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 263131.8901\n",
            "Epoch 44/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 292611.5584\n",
            "Epoch 45/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 261032.3282\n",
            "Epoch 46/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 262114.9079\n",
            "Epoch 47/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 253041.9962\n",
            "Epoch 48/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 257233.8905\n",
            "Epoch 49/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 300673.0511\n",
            "Epoch 50/50\n",
            "1092/1092 [==============================] - 4s 3ms/step - loss: 257509.6415\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeSlgF0c8oin",
        "colab_type": "code",
        "outputId": "a5761da0-b22b-49f2-e9d2-92610a1e9538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# summarize scores\n",
        "summarize_scores('lstm', score, scores)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lstm: [381.390] 378.5, 383.7, 343.2, 394.3, 373.5, 322.6, 459.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O_oqsuT8oiu",
        "colab_type": "code",
        "outputId": "03c68da5-d744-4a9d-a1c0-96bbfebd6c46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# plot scores\n",
        "days = ['sun', 'mon', 'tue', 'wed', 'thr', 'fri', 'sat']\n",
        "pyplot.plot(days, scores, marker='o', label='lstm')\n",
        "pyplot.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njYPCAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}